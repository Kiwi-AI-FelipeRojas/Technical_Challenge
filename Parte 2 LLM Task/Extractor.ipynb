{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfminer.high_level\n",
    "import docx\n",
    "import re\n",
    "import os\n",
    "import tiktoken\n",
    "from openai import AzureOpenAI    \n",
    "import ast\n",
    "import pandas as pd\n",
    "import math\n",
    "from io import StringIO\n",
    "import json\n",
    "\n",
    "#### Functions \n",
    "\n",
    "# Extract text from PDF documents\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Read text from PDF file\n",
    "    all_text = pdfminer.high_level.extract_text(pdf_path)\n",
    "    return all_text\n",
    "\n",
    "# Extract text from Word documents\n",
    "def extract_text_from_docx(pdf_path):\n",
    "    # open word file\n",
    "    doc = docx.Document(pdf_path)\n",
    "    # extract text \n",
    "    all_text = []\n",
    "    for para in doc.paragraphs:\n",
    "        all_text.append(para.text)\n",
    "    return '\\n'.join(all_text)\n",
    "\n",
    "def extract_text(pdf_path):\n",
    "    # read file \n",
    "    extension = os.path.splitext(pdf_path)[1]\n",
    "    if extension == '.pdf':\n",
    "        all_text = extract_text_from_pdf(pdf_path)\n",
    "    elif extension == '.docx':\n",
    "        all_text = extract_text_from_docx(pdf_path)\n",
    "    else:\n",
    "        all_text = 'Document type not supported'\n",
    "\n",
    "    return all_text\n",
    "\n",
    "#Function to preprocess the extracted text of the last version imported:\n",
    "def preprocess_text(text):\n",
    "    #Remove unwanted characters and line breaks:\n",
    "    text = re.sub(r'\\n+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "#Analyse text with LLM\n",
    "def LLM_extractor(client, text):\n",
    "    # train RAG model\n",
    "    response = client.chat.completions.create(\n",
    "        # GPT model\n",
    "        model = \"Team_Augmentation_4o_mini\",\n",
    "        # GPT model role\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"\"\"\n",
    "Intructions:\n",
    "1. Based on the given restauran order, extract the most relevant information of the order. \n",
    "2. Format the response as a Python dictionary.\n",
    "3. Ensure that all response is entirely in English, even if the input is in another language.\n",
    "4. Organice all keys of the dictionary as text with single quotes and avoid doing it as python lists.\n",
    "5. Adhere to the following structure:\n",
    "{   'Restaurant': 'Name of the restaurant',\n",
    "    'Order ID': 'All numerical and number sequence corresponfing to the ID of the order',\n",
    "    'Food item names': 'ordered item 1, ordered item 2, ordered item 3, ordered item 4',\n",
    "    'Actions taken': 'action 1, action 2, action 3, actions 4',\n",
    "    'Time of action': 'date and time the order was made',\n",
    "    'Total Price': 'total price of the order'\n",
    "},\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": text\n",
    "            }\n",
    "        ],\n",
    "        # set the parameters of the gpt model\n",
    "        temperature=0,\n",
    "        max_tokens=1000,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=[\"n/n\"]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Preprocess the response from GPT to delet unwanted characters\n",
    "def clean_response(text):\n",
    "    text = text.replace('```python', '').replace('```', '').strip()\n",
    "    text = text.replace(\"‘\", '\"').replace(\"’\", '\"')\n",
    "    text = text.replace(\"'\", '\"')\n",
    "    text = text.replace(\"`\", '\"')\n",
    "    text = re.sub(r'[^\\x00-\\x7f]+', ' ', text)\n",
    "    text = re.sub(r'(\"High_Level_Impression\": )([A-Za-z])', r'\\1\"\\2', text)\n",
    "    text = re.sub(r'(\"Location\": )([A-Za-z])', r'\\1\"\\2', text)\n",
    "    text = re.sub(r'\"s\\b', \"'s\", text)\n",
    "    text = re.split('{',text)[1]\n",
    "    text = re.split('}',text)[0]\n",
    "    text = '{' + text + '}'\n",
    "    text = ast.literal_eval(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Estimate number pf tokens\n",
    "def count_tokens(prompt: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    # Load the tokenizer corresponding to the specified model\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    # Encode the prompt to get the tokens\n",
    "    tokens = encoding.encode(prompt)\n",
    "    \n",
    "    return len(tokens)\n",
    "\n",
    "# Avarage calculation\n",
    "def calculate_mean(data):\n",
    "    # Calculate the mean of the list\n",
    "    return sum(data) / len(data)\n",
    "\n",
    "# Standard deviation calculation\n",
    "def calculate_standard_deviation(data):\n",
    "    # Calculate the mean\n",
    "    mean = calculate_mean(data)\n",
    "    \n",
    "    # Calculate the variance\n",
    "    variance = sum((x - mean) ** 2 for x in data) / len(data)\n",
    "    \n",
    "    # Return the square root of the variance (standard deviation)\n",
    "    return math.sqrt(variance)\n",
    "\n",
    "\n",
    "#Azure OpenAI conection \n",
    "# Import Azure Open AI credentials\n",
    "with open(\"Azure_credentials.txt\", \"r\") as file:\n",
    "    azure_endpoint = file.readline().strip()\n",
    "    api_key = file.readline().strip()\n",
    "    api_version = file.readline().strip()\n",
    "\n",
    "# Connect to Azure Open AI\n",
    "client = AzureOpenAI(azure_endpoint = azure_endpoint,\n",
    "                    api_key = api_key ,\n",
    "                    api_version = api_version)\n",
    "\n",
    "# Prompt used to train the RAG model\n",
    "prompt = \"\"\"\n",
    "Intructions:\n",
    "1. Based on the given restauran order, extract the most relevant information of the order. \n",
    "2. Format the response as a Python dictionary.\n",
    "3. Ensure that all response is entirely in English, even if the input is in another language.\n",
    "4. Organice all keys of the dictionary as text with single quotes and avoid doing it as python lists.\n",
    "5. Adhere to the following structure:\n",
    "{   'Restaurant': 'Name of the restaurant',\n",
    "    'Order ID': 'All numerical and number sequence corresponfing to the ID of the order',\n",
    "    'Food item names': 'ordered item 1, ordered item 2, ordered item 3, ordered item 4',\n",
    "    'Actions taken': 'action 1, action 2, action 3, actions 4',\n",
    "    'Time of action': 'date and time the order was made',\n",
    "    'Total Price': 'total price of the order'\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "# Declare varaibles where the results will be sotred \n",
    "results = []\n",
    "not_proccessed = {}\n",
    "tokens_candidate_in  = []\n",
    "tokens_candidate_out  = []\n",
    "\n",
    "#Path where the files are stored\n",
    "folder_path = 'Input_files'\n",
    "# Iterate over each of the files\n",
    "for filename in os.listdir(folder_path):\n",
    "    # get the file path\n",
    "    pdf_file_path = os.path.join(folder_path, filename)\n",
    "    try:\n",
    "        print(f\"Processing file: {filename}\")\n",
    "        # extract the text from the file\n",
    "        cv_text = extract_text(pdf_file_path)\n",
    "        if cv_text == 'Document type not supported':\n",
    "            print('Document type not supported')\n",
    "            not_proccessed[filename] = 'Document type not supported'\n",
    "        else:\n",
    "            # Preprocess the extracted text\n",
    "            cv_text_1 = preprocess_text(cv_text)\n",
    "            # calcuate the numbers of tokens that will be inputed to the model\n",
    "            prompt_text = prompt + cv_text_1\n",
    "            tokens_candidate_in.append(count_tokens(prompt_text, model=\"gpt-3.5-turbo\"))\n",
    "            # Extract the information using the LLM\n",
    "            cv_text_2 = LLM_extractor(client, cv_text_1)\n",
    "            # Get the nomber of tokens of the response\n",
    "            tokens_candidate_out.append(count_tokens(cv_text_2, model=\"gpt-3.5-turbo\"))\n",
    "            # Clean the LLM response\n",
    "            text2 = clean_response(cv_text_2)\n",
    "            results.append(text2)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        not_proccessed[filename] = e\n",
    "\n",
    "#Estimate mean and std deviation of the tokens \n",
    "mean_tokens_candidate_in = calculate_mean(tokens_candidate_in)\n",
    "std_dev_tokens_candidate_in = calculate_standard_deviation(tokens_candidate_in)\n",
    "mean_tokens_candidate_out = calculate_mean(tokens_candidate_out)\n",
    "std_dev_tokens_candidate_out = calculate_standard_deviation(tokens_candidate_out)\n",
    "\n",
    "print('Mean number of tokens per document in: {} with a standard deviation of: {}'.format(mean_tokens_candidate_in, std_dev_tokens_candidate_in))\n",
    "print('Mean number of tokens per document out: {} with a standard deviation of: {}'.format(mean_tokens_candidate_out, std_dev_tokens_candidate_out))\n",
    "\n",
    "# Convert results to dataframe\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "# Convert to JSON\n",
    "json_data = df.to_json(orient=\"records\", indent=4)\n",
    "# Export jason file \n",
    "df.to_json(\"extractor_results.txt\", orient=\"records\", indent=4)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NextantCVs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
